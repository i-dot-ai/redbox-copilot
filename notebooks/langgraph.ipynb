{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redbox.graph.root import get_redbox_graph, run_redbox\n",
    "from redbox.graph.chat import get_chat_with_docs_graph\n",
    "from redbox.chains import components\n",
    "from redbox.models.settings import Settings, ElasticLocalSettings, AISettings\n",
    "from redbox.models.chain import ChainInput, ChainState\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import tiktoken\n",
    "\n",
    "_ = load_dotenv(Path.cwd().parent / \".env\")\n",
    "\n",
    "ENV = Settings(\n",
    "    minio_host=\"localhost\", \n",
    "    object_store=\"minio\", \n",
    "    azure_openai_model=\"gpt-4o\",\n",
    "    elastic=ElasticLocalSettings(host=\"localhost\"),\n",
    ")\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    api_key=ENV.azure_openai_api_key,\n",
    "    azure_endpoint=ENV.azure_openai_endpoint,\n",
    "    model=ENV.azure_openai_model,\n",
    ")\n",
    "\n",
    "AI_CONFIG = {\"configurable\": ENV.ai.dict()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import UUID\n",
    "from langchain_core.documents import Document\n",
    "from redbox.models.chat import ChatRoute\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "type DocumentState = dict[UUID, dict[UUID, Document]]\n",
    "\n",
    "def document_reducer(left: DocumentState, right: DocumentState) -> DocumentState:\n",
    "    \"\"\"Merges two document states based on the following rules.\n",
    "    \n",
    "    * Groups are matched by the group key.\n",
    "    * Documents are matched by the group key and document key.\n",
    "\n",
    "    Then:\n",
    "    \n",
    "    * If key(s) are matched, the group or Document is replaced\n",
    "    * If key(s) are matched and the key is None, the key is cleared\n",
    "    * If key(s) aren't matched, group or Document is added\n",
    "    \"\"\"\n",
    "    # If state is empty, return right\n",
    "    if left is None:\n",
    "        return right\n",
    "    \n",
    "    # Copy left\n",
    "    reduced = {k: v.copy() for k, v in left.items()}\n",
    "    \n",
    "    # Update with right\n",
    "    for group_key, group in right.items():\n",
    "        # If group is None, remove from output if a group key is matched\n",
    "        if group is None:\n",
    "            reduced.pop(group_key, None)\n",
    "            continue\n",
    "        \n",
    "        # If group key isn't matched, add it\n",
    "        if group_key not in reduced:\n",
    "            reduced[group_key] = group\n",
    "\n",
    "        \n",
    "        for document_key, document in group.items():\n",
    "            if document is None:\n",
    "                # If Document is None, remove from output if a group and document key is matched\n",
    "                reduced[group_key].pop(document_key, None)\n",
    "            else:\n",
    "                # Otherwise, update or add the value\n",
    "                reduced[group_key][document_key] = document\n",
    "\n",
    "        # Remove group_key from output if it becomes empty after updates\n",
    "        if not reduced[group_key]:\n",
    "            del reduced[group_key]\n",
    "\n",
    "    return reduced\n",
    "\n",
    "class State(TypedDict):\n",
    "    request: ChainInput\n",
    "    text: str | None\n",
    "    documents: Annotated[DocumentState, document_reducer] | None\n",
    "    route: ChatRoute | None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = {\n",
    "    UUID(\"00000000-0000-0000-0000-000000000001\"): {\n",
    "        UUID(\"10000000-0000-0000-0000-000000000000\"): Document(page_content=\"a\"),\n",
    "        UUID(\"20000000-0000-0000-0000-000000000000\"): Document(page_content=\"b\")\n",
    "    },\n",
    "    UUID(\"00000000-0000-0000-0000-000000000002\"): {\n",
    "        UUID(\"30000000-0000-0000-0000-000000000000\"): Document(page_content=\"c\"),\n",
    "        UUID(\"40000000-0000-0000-0000-000000000000\"): Document(page_content=\"d\")\n",
    "    }\n",
    "}\n",
    "\n",
    "y = {\n",
    "    UUID(\"00000000-0000-0000-0000-000000000001\"): {\n",
    "        UUID(\"10000000-0000-0000-0000-000000000000\"): Document(page_content=\"z\"),\n",
    "        UUID(\"20000000-0000-0000-0000-000000000000\"): None,\n",
    "        UUID(\"30000000-0000-0000-0000-000000000000\"): Document(page_content=\"c\")\n",
    "    },\n",
    "    UUID(\"00000000-0000-0000-0000-000000000002\"): None\n",
    "}\n",
    "\n",
    "z = {\n",
    "    UUID(\"00000000-0000-0000-0000-000000000001\"): {\n",
    "        UUID(\"10000000-0000-0000-0000-000000000000\"): Document(page_content=\"z\"),\n",
    "        UUID(\"30000000-0000-0000-0000-000000000000\"): Document(page_content=\"c\")\n",
    "    }\n",
    "}\n",
    "\n",
    "a = document_reducer(x, y)\n",
    "\n",
    "a == z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import re\n",
    "\n",
    "def calculate_token_budget(state: State, config: AISettings) -> int:\n",
    "    tokeniser = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    len_question = len(tokeniser.encode(state[\"request\"].question))\n",
    "    max_system_or_question_prompt = max(\n",
    "        [\n",
    "            len(tokeniser.encode(v))\n",
    "            for k, v in config[\"configurable\"].items() \n",
    "            if isinstance(v, str)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    len_prompts = max_system_or_question_prompt * 2\n",
    "    # config.llm_max_tokens is 1024 -- move max tokens to AISettings\n",
    "    len_max_output = 1_024\n",
    "    \n",
    "    return config[\"configurable\"][\"context_window_size\"] - len_max_output - len_prompts - len_question\n",
    "\n",
    "def conditional_doceuments_bigger_than_context(state: State, config: AISettings) -> bool:\n",
    "    token_budget = calculate_token_budget(state=state, config=config)\n",
    "\n",
    "    if sum(d.metadata[\"token_count\"] for d in state[\"documents\"]) > token_budget:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def conditional_keyword_detection(state: State, config: AISettings) -> ChatRoute | None:\n",
    "    re_keyword_pattern = re.compile(r\"@(\\w+)\")\n",
    "\n",
    "    route_match = re_keyword_pattern.search(state[\"request\"].question)\n",
    "    route_name = route_match.group()[1:] if route_match else None\n",
    "    \n",
    "    try:\n",
    "        return ChatRoute[route_name]\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "def conditional_documents_selected(state: State, config: AISettings) -> bool:\n",
    "    return len(state[\"request\"].file_uuids) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "from typing import Callable\n",
    "\n",
    "def make_document_group_send(target: str) -> Callable[[State, AISettings], list[Send]]:\n",
    "    def _group_send(state: State, config: AISettings) -> list[Send]:\n",
    "        group_send_states: list[State] = [\n",
    "            State(\n",
    "                request=state[\"request\"],\n",
    "                text=state[\"text\"],\n",
    "                documents={group_key: state[\"documents\"][group_key]},\n",
    "                route=state[\"route\"]\n",
    "            )\n",
    "            for group_key in state[\"documents\"]\n",
    "        ]\n",
    "        return [Send(node=target, arg=state) for state in group_send_states]\n",
    "    return _group_send\n",
    "\n",
    "def make_document_chunk_send(target: str) -> Callable[[State, AISettings], list[Send]]:\n",
    "    def _chunk_send(state: State, config: AISettings) -> list[Send]:\n",
    "        chunk_send_states: list[State] = [\n",
    "            State(\n",
    "                request=state[\"request\"],\n",
    "                text=state[\"text\"],\n",
    "                documents={group_key: {document_key: state[\"documents\"][group_key][document_key]}},\n",
    "                route=state[\"route\"]\n",
    "            )\n",
    "            for group_key in state[\"documents\"]\n",
    "            for document_key in state[\"documents\"][group_key]\n",
    "        ]\n",
    "        return [Send(node=target, arg=state) for state in chunk_send_states]\n",
    "    return _chunk_send\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_document_state(documents: dict[UUID, dict[UUID, Document]]) -> list[Document]:\n",
    "    return [\n",
    "        document\n",
    "        for group in documents.values()\n",
    "        for document in group.values()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'request': {'question': 'Who are you?', 'file_uuids': [], 'user_uuid': UUID('a16b2762-fc18-4595-bafa-3d70f1e85087'), 'chat_history': []}, 'text': None, 'documents': None, 'route': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': \"Arrr matey! I be a swashbucklin' pirate, scourin' the seven seas in search o' treasure and adventure! What can I do fer ye today?\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "from redbox.api.runnables import make_chat_prompt_from_messages_runnable\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "def make_chat_pattern(\n",
    "    system_prompt: str,\n",
    "    question_prompt: str,\n",
    "    llm: BaseChatModel\n",
    ") -> Callable[[State, AISettings], dict[str, Any]]:\n",
    "    \"\"\"Returns a function that uses state[\"request\"] to set state[\"text\"].\"\"\"\n",
    "    def _chat(state: State, config: AISettings) -> dict[str, Any]:\n",
    "        print(state)\n",
    "        chat_chain = (\n",
    "            make_chat_prompt_from_messages_runnable(\n",
    "                system_prompt=system_prompt,\n",
    "                question_prompt=question_prompt,\n",
    "                input_token_budget=config[\"configurable\"][\"context_window_size\"] - 1_024,\n",
    "                tokeniser=tiktoken.get_encoding(\"cl100k_base\"),\n",
    "            )\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"text\": chat_chain.invoke(state[\"request\"]),\n",
    "        }\n",
    "\n",
    "    return _chat\n",
    "\n",
    "chat_state = State(\n",
    "    request=ChainInput(\n",
    "        question=\"Who are you?\",\n",
    "        file_uuids=[],\n",
    "        user_uuid=\"a16b2762-fc18-4595-bafa-3d70f1e85087\",\n",
    "        chat_history=[],\n",
    "    ).dict(),\n",
    "    text=None,\n",
    "    documents=None,\n",
    "    route=None\n",
    ")\n",
    "\n",
    "chat_node = make_chat_pattern(\n",
    "    system_prompt=\"You're a pirate.\", \n",
    "    question_prompt=\"{question}\\n\\n Response: \",\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "chat_node(chat_state, AI_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'The animals named in the documents are:\\n\\n1. Dog\\n2. Cat\\n3. Swift\\n4. Shrike'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough \n",
    "\n",
    "from redbox.api.format import format_documents\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "def make_stuff_pattern(\n",
    "    system_prompt: str,\n",
    "    question_prompt: str,\n",
    "    llm: BaseChatModel,\n",
    ") -> Callable[[State, AISettings], dict[str, Any]]:\n",
    "    \"\"\"Returns a function that uses state[\"request\"] and state[\"documents\"] to set state[\"text\"].\"\"\"\n",
    "    def _stuff(state: State, config: AISettings) -> dict[str, Any]:\n",
    "        stuff_chain = (\n",
    "            RunnablePassthrough.assign(\n",
    "                documents=(\n",
    "                    RunnablePassthrough() \n",
    "                    | itemgetter(\"documents\") \n",
    "                    | flatten_document_state \n",
    "                    | format_documents\n",
    "                )\n",
    "            ) \n",
    "            | make_chat_prompt_from_messages_runnable(\n",
    "                system_prompt=system_prompt,\n",
    "                question_prompt=question_prompt,\n",
    "                input_token_budget=config[\"configurable\"][\"context_window_size\"] - 1_024,\n",
    "                tokeniser=tiktoken.get_encoding(\"cl100k_base\"),\n",
    "            )\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"text\": stuff_chain.invoke({\n",
    "                **state[\"request\"],\n",
    "                \"documents\": state[\"documents\"]\n",
    "            })\n",
    "        }\n",
    "\n",
    "    return _stuff\n",
    "\n",
    "stuff_state = State(\n",
    "    request=ChainInput(\n",
    "        question=\"What animals are named in these documents?\",\n",
    "        file_uuids=[\"71b76e2a-9c2f-4340-ba94-cc09c7de74ff\"],\n",
    "        user_uuid=\"a16b2762-fc18-4595-bafa-3d70f1e85087\",\n",
    "        chat_history=[],\n",
    "    ).dict(),\n",
    "    text=None,\n",
    "    documents={\n",
    "        UUID(\"00000000-0000-0000-0000-000000000001\"): {\n",
    "            UUID(\"10000000-0000-0000-0000-000000000000\"): Document(\n",
    "                page_content=\"A dog is a type of mammal.\",\n",
    "                metadata={\n",
    "                    \"uuid\": UUID(\"10000000-0000-0000-0000-000000000000\"),\n",
    "                    \"parent_file_uuid\": UUID(\"00000000-0000-0000-0000-000000000001\"),\n",
    "                    \"token_count\": 6,\n",
    "                }\n",
    "            ),\n",
    "            UUID(\"20000000-0000-0000-0000-000000000000\"): Document(\n",
    "                page_content=\"A cat is a type of mammal.\",\n",
    "                metadata={\n",
    "                    \"uuid\": UUID(\"20000000-0000-0000-0000-000000000000\"),\n",
    "                    \"parent_file_uuid\": UUID(\"00000000-0000-0000-0000-000000000001\"),\n",
    "                    \"token_count\": 6,\n",
    "                }\n",
    "            )\n",
    "        },\n",
    "        UUID(\"00000000-0000-0000-0000-000000000002\"): {\n",
    "            UUID(\"30000000-0000-0000-0000-000000000000\"): Document(\n",
    "                page_content=\"A swift is a type of bird.\",\n",
    "                metadata={\n",
    "                    \"uuid\": UUID(\"30000000-0000-0000-0000-000000000000\"),\n",
    "                    \"parent_file_uuid\": UUID(\"00000000-0000-0000-0000-000000000002\"),\n",
    "                    \"token_count\": 6,\n",
    "                }\n",
    "            ),\n",
    "            UUID(\"40000000-0000-0000-0000-000000000000\"): Document(\n",
    "                page_content=\"A shrike is a type of bird.\",\n",
    "                metadata={\n",
    "                    \"uuid\": UUID(\"40000000-0000-0000-0000-000000000000\"),\n",
    "                    \"parent_file_uuid\": UUID(\"00000000-0000-0000-0000-000000000002\"),\n",
    "                    \"token_count\": 6,\n",
    "                }\n",
    "            )\n",
    "        }\n",
    "    },\n",
    "    route=None\n",
    ")\n",
    "\n",
    "stuff_node = make_stuff_pattern(\n",
    "    system_prompt=\"Summarise these documents.\", \n",
    "    question_prompt=\"Question: {question}. \\n\\n Documents: \\n\\n {documents} \\n\\n Answer:\",\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "stuff_node(stuff_state, AI_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': {UUID('00000000-0000-0000-0000-000000000001'): {UUID('10000000-0000-0000-0000-000000000000'): Document(metadata={'uuid': UUID('10000000-0000-0000-0000-000000000000'), 'parent_file_uuid': UUID('00000000-0000-0000-0000-000000000001'), 'token_count': 24, 'page_number': None, 'languages': None, 'link_texts': None, 'link_urls': None, 'links': None}, page_content='The animals named in the documents are:\\n\\n1. Dog\\n2. Cat\\n3. Swift\\n4. Shrike')}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from redbox.transform import combine_documents\n",
    "\n",
    "from functools import reduce\n",
    "from uuid import uuid4\n",
    "\n",
    "def make_merge_pattern(\n",
    "    system_prompt: str,\n",
    "    question_prompt: str,\n",
    "    llm: BaseChatModel,\n",
    ") -> Callable[[State, AISettings], dict[str, Any]]:\n",
    "    \"\"\"Returns a function that uses state[\"request\"] and state[\"documents\"] to return one item in state[\"documents\"].\n",
    "    \n",
    "    When combined with chunk send, will replace each Document with what's returned from the LLM.\n",
    "\n",
    "    When combined with group end, with combine all Documents and use the metadata of the first.\n",
    "\n",
    "    When used without a send, the first Document receieved defines the metadata.\n",
    "    \"\"\"\n",
    "    def _merge(state: State, config: AISettings) -> dict[str, Any]:\n",
    "        tokeniser = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "        flattened_documents = flatten_document_state(state[\"documents\"])\n",
    "        \n",
    "        merged_document = reduce(lambda l, r: combine_documents(l, r), flattened_documents)\n",
    "        \n",
    "        stuff_chain = (\n",
    "            RunnablePassthrough.assign(\n",
    "                documents=(\n",
    "                    RunnablePassthrough() \n",
    "                    | itemgetter(\"documents\") \n",
    "                    | flatten_document_state \n",
    "                    | format_documents\n",
    "                )\n",
    "            ) \n",
    "            | make_chat_prompt_from_messages_runnable(\n",
    "                system_prompt=system_prompt,\n",
    "                question_prompt=question_prompt,\n",
    "                input_token_budget=config[\"configurable\"][\"context_window_size\"] - 1_024,\n",
    "                tokeniser=tokeniser,\n",
    "            )\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        merged_document.page_content = stuff_chain.invoke({\n",
    "            **state[\"request\"],\n",
    "            \"documents\": state[\"documents\"]\n",
    "        })\n",
    "        merged_document.metadata[\"token_count\"] = len(\n",
    "            tokeniser.encode(merged_document.page_content)\n",
    "        )\n",
    "\n",
    "        group_uuid = merged_document.metadata.get(\"parent_file_uuid\", uuid4())\n",
    "        chunk_uuid = merged_document.metadata.get(\"uuid\", uuid4())\n",
    "        \n",
    "        return {\n",
    "            \"documents\": { group_uuid: { chunk_uuid: merged_document } }\n",
    "        }\n",
    "\n",
    "    return _merge\n",
    "\n",
    "merge_state = State(\n",
    "    request=ChainInput(\n",
    "        question=\"What animals are named in these documents?\",\n",
    "        file_uuids=[\"71b76e2a-9c2f-4340-ba94-cc09c7de74ff\"],\n",
    "        user_uuid=\"a16b2762-fc18-4595-bafa-3d70f1e85087\",\n",
    "        chat_history=[],\n",
    "    ).dict(),\n",
    "    text=None,\n",
    "    documents={\n",
    "        UUID(\"00000000-0000-0000-0000-000000000001\"): {\n",
    "            UUID(\"10000000-0000-0000-0000-000000000000\"): Document(\n",
    "                page_content=\"A dog is a type of mammal.\",\n",
    "                metadata={\n",
    "                    \"uuid\": UUID(\"10000000-0000-0000-0000-000000000000\"),\n",
    "                    \"parent_file_uuid\": UUID(\"00000000-0000-0000-0000-000000000001\"),\n",
    "                    \"token_count\": 6,\n",
    "                }\n",
    "            ),\n",
    "            UUID(\"20000000-0000-0000-0000-000000000000\"): Document(\n",
    "                page_content=\"A cat is a type of mammal.\",\n",
    "                metadata={\n",
    "                    \"uuid\": UUID(\"20000000-0000-0000-0000-000000000000\"),\n",
    "                    \"parent_file_uuid\": UUID(\"00000000-0000-0000-0000-000000000001\"),\n",
    "                    \"token_count\": 6,\n",
    "                }\n",
    "            )\n",
    "        },\n",
    "        UUID(\"00000000-0000-0000-0000-000000000002\"): {\n",
    "            UUID(\"30000000-0000-0000-0000-000000000000\"): Document(\n",
    "                page_content=\"A swift is a type of bird.\",\n",
    "                metadata={\n",
    "                    \"uuid\": UUID(\"30000000-0000-0000-0000-000000000000\"),\n",
    "                    \"parent_file_uuid\": UUID(\"00000000-0000-0000-0000-000000000002\"),\n",
    "                    \"token_count\": 6,\n",
    "                }\n",
    "            ),\n",
    "            UUID(\"40000000-0000-0000-0000-000000000000\"): Document(\n",
    "                page_content=\"A shrike is a type of bird.\",\n",
    "                metadata={\n",
    "                    \"uuid\": UUID(\"40000000-0000-0000-0000-000000000000\"),\n",
    "                    \"parent_file_uuid\": UUID(\"00000000-0000-0000-0000-000000000002\"),\n",
    "                    \"token_count\": 6,\n",
    "                }\n",
    "            )\n",
    "        }\n",
    "    },\n",
    "    route=None\n",
    ")\n",
    "\n",
    "merge_node = make_merge_pattern(\n",
    "    system_prompt=\"Summarise these documents.\", \n",
    "    question_prompt=\"Question: {question}. \\n\\n Documents: \\n\\n {documents} \\n\\n Answer:\",\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "merge_node(merge_state, AI_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:00000000-0000-0000-0000-000000000001\n",
      "Document:10000000-0000-0000-0000-000000000000\n",
      "Document:20000000-0000-0000-0000-000000000000\n",
      "---\n",
      "Group:00000000-0000-0000-0000-000000000002\n",
      "Document:30000000-0000-0000-0000-000000000000\n",
      "Document:40000000-0000-0000-0000-000000000000\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def make_retrieve_pattern(\n",
    "    retriever: BaseRetriever\n",
    ") -> Callable[[State, AISettings], dict[str, Any]]:\n",
    "    \"\"\"Returns a function that uses state[\"request\"] and state[\"text\"] to set state[\"documents\"].\"\"\"\n",
    "    def _retrieve(state: State, config: AISettings) -> dict[str, Any]:\n",
    "        document_states: list[DocumentState] = []\n",
    "        request = state[\"request\"]\n",
    "        request[\"question\"] = state[\"text\"]\n",
    "\n",
    "        for document in retriever.invoke(request):\n",
    "            group_uuid = document.metadata.get(\"parent_file_uuid\", uuid4())\n",
    "            chunk_uuid = document.metadata.get(\"uuid\", uuid4())\n",
    "            document_states.append({ group_uuid: { chunk_uuid: document } })\n",
    "    \n",
    "        return {\n",
    "            \"documents\": reduce(lambda l, r: document_reducer(l, r), document_states)\n",
    "        }\n",
    "\n",
    "    return _retrieve\n",
    "\n",
    "\n",
    "class FakeRetriever(BaseRetriever):\n",
    "    docs: list[Document]\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> list[Document]:\n",
    "        return self.docs\n",
    "\n",
    "    async def _aget_relevant_documents(self, query: str) -> list[Document]:\n",
    "        return self.docs\n",
    "\n",
    "\n",
    "fake_retriever = FakeRetriever(\n",
    "    docs=[\n",
    "        Document(\n",
    "            page_content=\"A dog is a type of mammal.\",\n",
    "            metadata={\n",
    "                \"uuid\": UUID(\"10000000-0000-0000-0000-000000000000\"),\n",
    "                \"parent_file_uuid\": UUID(\"00000000-0000-0000-0000-000000000001\"),\n",
    "                \"token_count\": 6,\n",
    "            }\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=\"A cat is a type of mammal.\",\n",
    "            metadata={\n",
    "                \"uuid\": UUID(\"20000000-0000-0000-0000-000000000000\"),\n",
    "                \"parent_file_uuid\": UUID(\"00000000-0000-0000-0000-000000000001\"),\n",
    "                \"token_count\": 6,\n",
    "            }\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=\"A swift is a type of bird.\",\n",
    "            metadata={\n",
    "                \"uuid\": UUID(\"30000000-0000-0000-0000-000000000000\"),\n",
    "                \"parent_file_uuid\": UUID(\"00000000-0000-0000-0000-000000000002\"),\n",
    "                \"token_count\": 6,\n",
    "            }\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=\"A shrike is a type of bird.\",\n",
    "            metadata={\n",
    "                \"uuid\": UUID(\"40000000-0000-0000-0000-000000000000\"),\n",
    "                \"parent_file_uuid\": UUID(\"00000000-0000-0000-0000-000000000002\"),\n",
    "                \"token_count\": 6,\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieve_state = State(\n",
    "    request=ChainInput(\n",
    "        question=\"What are some animals?\",\n",
    "        file_uuids=[\"71b76e2a-9c2f-4340-ba94-cc09c7de74ff\"],\n",
    "        user_uuid=\"a16b2762-fc18-4595-bafa-3d70f1e85087\",\n",
    "        chat_history=[],\n",
    "    ).dict(),\n",
    "    text=None,\n",
    "    documents=None,\n",
    "    route=None\n",
    ")\n",
    "\n",
    "retrieve_node = make_retrieve_pattern(retriever=fake_retriever)\n",
    "\n",
    "document_state = retrieve_node(retrieve_state, AI_CONFIG)\n",
    "\n",
    "for group in document_state[\"documents\"]:\n",
    "    print(f\"Group:{group}\")\n",
    "    for document in document_state[\"documents\"][group]:\n",
    "        print(f\"Document:{document}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'request': None, 'text': None, 'documents': None, 'route': None}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 52\u001b[0m\n\u001b[1;32m     38\u001b[0m rag_compiled \u001b[38;5;241m=\u001b[39m rag\u001b[38;5;241m.\u001b[39mcompile()\n\u001b[1;32m     40\u001b[0m rag_state \u001b[38;5;241m=\u001b[39m State(\n\u001b[1;32m     41\u001b[0m     request\u001b[38;5;241m=\u001b[39mChainInput(\n\u001b[1;32m     42\u001b[0m         question\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are some animals?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     route\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     50\u001b[0m )\n\u001b[0;32m---> 52\u001b[0m \u001b[43mrag_compiled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrag_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAI_CONFIG\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/redbox-VIdz9l6D-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1263\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1262\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1263\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/redbox-VIdz9l6D-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py:948\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[1;32m    947\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 948\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/redbox-VIdz9l6D-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1349\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step, timeout_exc_cls)\u001b[0m\n\u001b[1;32m   1347\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1348\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1352\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1354\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/redbox-VIdz9l6D-py3.12/lib/python3.12/site-packages/langgraph/pregel/executor.py:60\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m         \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/redbox-VIdz9l6D-py3.12/lib/python3.12/site-packages/langgraph/pregel/retry.py:25\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     23\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/redbox-VIdz9l6D-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py:2873\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2869\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m   2870\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2871\u001b[0m )\n\u001b[1;32m   2872\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2873\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2875\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/redbox-VIdz9l6D-py3.12/lib/python3.12/site-packages/langgraph/utils.py:102\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m    101\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 102\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m, in \u001b[0;36mmake_chat_pattern.<locals>._chat\u001b[0;34m(state, config)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(state)\n\u001b[1;32m     17\u001b[0m chat_chain \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     18\u001b[0m     make_chat_prompt_from_messages_runnable(\n\u001b[1;32m     19\u001b[0m         system_prompt\u001b[38;5;241m=\u001b[39msystem_prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mchat_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrequest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     30\u001b[0m }\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/redbox-VIdz9l6D-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py:2873\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2869\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m   2870\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2871\u001b[0m )\n\u001b[1;32m   2872\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2873\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2875\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/redbox-VIdz9l6D-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py:4438\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4424\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[1;32m   4425\u001b[0m \n\u001b[1;32m   4426\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4435\u001b[0m \u001b[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[1;32m   4436\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 4438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4439\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4440\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4441\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4442\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4443\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4444\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   4446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4447\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4448\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/redbox-VIdz9l6D-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py:1784\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1781\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1782\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1783\u001b[0m         Output,\n\u001b[0;32m-> 1784\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1787\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1792\u001b[0m     )\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1794\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/redbox-VIdz9l6D-py3.12/lib/python3.12/site-packages/langchain_core/runnables/config.py:404\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    403\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/redbox-VIdz9l6D-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py:4294\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   4292\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   4293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4294\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4295\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   4296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4297\u001b[0m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[1;32m   4298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/redbox-VIdz9l6D-py3.12/lib/python3.12/site-packages/langchain_core/runnables/config.py:404\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    403\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DS/redbox/redbox-core/redbox/api/runnables.py:36\u001b[0m, in \u001b[0;36mmake_chat_prompt_from_messages_runnable.<locals>.chat_prompt_from_messages\u001b[0;34m(input_dict)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;129m@chain\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat_prompt_from_messages\u001b[39m(input_dict: \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    Create a ChatPrompTemplate as part of a chain using 'chat_history'.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    Returns the PromptValue using values in the input_dict\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     chat_history_budget \u001b[38;5;241m=\u001b[39m token_budget \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokeniser\u001b[38;5;241m.\u001b[39mencode(\u001b[43minput_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chat_history_budget \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m QuestionLengthError\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "rag = StateGraph(ChainState)\n",
    "\n",
    "_chat = make_chat_pattern(\n",
    "    system_prompt=\"Write the following chat as a single question.\", \n",
    "    question_prompt=\"{question}\\n\\n Single question: \",\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "rag.add_node(\n",
    "    \"rag_chat\", _chat\n",
    "    # make_chat_pattern(\n",
    "    #     system_prompt=\"Write the following chat as a single question.\", \n",
    "    #     question_prompt=\"{question}\\n\\n Single question: \",\n",
    "    #     llm=llm\n",
    "    # )\n",
    ")\n",
    "# rag.add_node(\n",
    "#     \"rag_retrieve\", \n",
    "#     make_retrieve_pattern(retriever=fake_retriever)\n",
    "# )\n",
    "# rag.add_node(\n",
    "#     \"rag_stuff\",\n",
    "#     make_stuff_pattern(\n",
    "#         system_prompt=\"Summarise these documents.\", \n",
    "#         question_prompt=\"Question: {question}. \\n\\n Documents: \\n\\n {documents} \\n\\n Answer:\",\n",
    "#         llm=llm\n",
    "#     )\n",
    "# )\n",
    "\n",
    "rag.add_edge(START, \"rag_chat\")\n",
    "# rag.add_edge(\"rag_chat\", \"rag_retrieve\")\n",
    "# rag.add_edge(\"rag_retrieve\", \"rag_stuff\")\n",
    "# rag.add_edge(\"rag_stuff\", END)\n",
    "rag.add_edge(\"rag_chat\", END)\n",
    "\n",
    "rag_compiled = rag.compile()\n",
    "\n",
    "rag_state = State(\n",
    "    request=ChainInput(\n",
    "        question=\"What are some animals?\",\n",
    "        file_uuids=[\"71b76e2a-9c2f-4340-ba94-cc09c7de74ff\"],\n",
    "        user_uuid=\"a16b2762-fc18-4595-bafa-3d70f1e85087\",\n",
    "        chat_history=[],\n",
    "    ).dict(),\n",
    "    text=None,\n",
    "    documents=None,\n",
    "    route=None\n",
    ")\n",
    "\n",
    "rag_compiled.invoke(rag_state, config=AI_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'request': {'question': 'What are some animals?',\n",
       "  'file_uuids': [UUID('71b76e2a-9c2f-4340-ba94-cc09c7de74ff')],\n",
       "  'user_uuid': UUID('a16b2762-fc18-4595-bafa-3d70f1e85087'),\n",
       "  'chat_history': []},\n",
       " 'text': None,\n",
       " 'documents': None,\n",
       " 'route': None}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from redbox.models import ChatRoute\n",
    "\n",
    "chat_app = StateGraph(ChainState)\n",
    "\n",
    "app = StateGraph(ChainState)\n",
    "app.set_entry_point(\"set_route\")\n",
    "\n",
    "app.add_node(ChatRoute.chat, get_chat_graph(llm, _tokeniser, _env, debug))\n",
    "app.add_edge(ChatRoute.chat, END)\n",
    "\n",
    "app.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redbox.chains.components import get_all_chunks_retriever\n",
    "from redbox.api.format import format_documents\n",
    "\n",
    "x = get_all_chunks_retriever(ENV) | format_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.invoke(\n",
    "    ChainState(\n",
    "        query=ChainInput(\n",
    "            question=\"What are Labour's five key missions?\",\n",
    "            # file_uuids=[],\n",
    "            file_uuids=[\"71b76e2a-9c2f-4340-ba94-cc09c7de74ff\"],\n",
    "            user_uuid=\"a16b2762-fc18-4595-bafa-3d70f1e85087\",\n",
    "            chat_history=[],\n",
    "        )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.invoke(\n",
    "    ChainState(\n",
    "        query=ChainInput(\n",
    "            question=\"What are Labour's five missions?\",\n",
    "            # file_uuids=[],\n",
    "            file_uuids=[\"71b76e2a-9c2f-4340-ba94-cc09c7de74ff\"],\n",
    "            user_uuid=\"a16b2762-fc18-4595-bafa-3d70f1e85087\",\n",
    "            chat_history=[],\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for i in app.astream_events(\n",
    "    ChainState(\n",
    "        query=ChainInput(\n",
    "            question=\"What are Labour's five missions?\",\n",
    "            # file_uuids=[],\n",
    "            file_uuids=[\"71b76e2a-9c2f-4340-ba94-cc09c7de74ff\"],\n",
    "            user_uuid=\"a16b2762-fc18-4595-bafa-3d70f1e85087\",\n",
    "            chat_history=[],\n",
    "        )\n",
    "    ),\n",
    "    version=\"v2\"\n",
    "):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redbox.graph.chat import get_chat_with_docs_graph\n",
    "from redbox.chains.components import get_all_chunks_retriever\n",
    "\n",
    "cwd_app = get_chat_with_docs_graph(\n",
    "    llm=llm,\n",
    "    tokeniser=tiktoken.get_encoding(\"cl100k_base\"),\n",
    "    all_chunks_retriever=get_all_chunks_retriever(ENV),\n",
    "    env=ENV\n",
    ")\n",
    "\n",
    "cwd_app.invoke(\n",
    "    ChainState(\n",
    "        query=ChainInput(\n",
    "            question=\"What's the filename of this document?\",\n",
    "            # file_uuids=[],\n",
    "            file_uuids=[\"71b76e2a-9c2f-4340-ba94-cc09c7de74ff\"],\n",
    "            user_uuid=\"a16b2762-fc18-4595-bafa-3d70f1e85087\",\n",
    "            chat_history=[],\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redbox.graph.chat import get_chat_graph\n",
    "\n",
    "cwd_app = get_chat_graph(\n",
    "    llm=llm,\n",
    "    tokeniser=tiktoken.get_encoding(\"cl100k_base\"),\n",
    "    env=ENV\n",
    ")\n",
    "\n",
    "cwd_app.invoke(\n",
    "    ChainState(\n",
    "        query=ChainInput(\n",
    "            question=\"What's the filename of this document?\",\n",
    "            # file_uuids=[],\n",
    "            file_uuids=[\"71b76e2a-9c2f-4340-ba94-cc09c7de74ff\"],\n",
    "            user_uuid=\"a16b2762-fc18-4595-bafa-3d70f1e85087\",\n",
    "            chat_history=[],\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redbox.chains.graph import build_llm_chain\n",
    "\n",
    "llm_chain = build_llm_chain(\n",
    "    llm=llm, \n",
    "    tokeniser=tiktoken.get_encoding(\"cl100k_base\"), \n",
    "    env=ENV, \n",
    "    system_prompt=ENV.ai.chat_system_prompt, \n",
    "    question_prompt=ENV.ai.chat_question_prompt\n",
    ")\n",
    "\n",
    "llm_chain.invoke(\n",
    "    ChainState(\n",
    "        query=ChainInput(\n",
    "            question=\"What's the filename of this document?\",\n",
    "            # file_uuids=[],\n",
    "            file_uuids=[\"71b76e2a-9c2f-4340-ba94-cc09c7de74ff\"],\n",
    "            user_uuid=\"a16b2762-fc18-4595-bafa-3d70f1e85087\",\n",
    "            chat_history=[]\n",
    "        ),\n",
    "        formatted_documents=\"cool.txt\"\n",
    "    ),\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "env = Settings()\n",
    "chat_graph = get_chat_with_docs_graph(\n",
    "    llm=components.get_chat_llm(env),\n",
    "    all_chunks_retriever=components.get_all_chunks_retriever(env),\n",
    "    tokeniser=components.get_tokeniser(),\n",
    "    env=env\n",
    ")\n",
    "display(\n",
    "    Image(\n",
    "        chat_graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

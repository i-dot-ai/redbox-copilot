{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from typing import List, TYPE_CHECKING\n",
    "from functools import reduce, wraps\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "from pydantic import Field\n",
    "from faststream.redis.fastapi import RedisRouter\n",
    "from elasticsearch import Elasticsearch\n",
    "from langchain.chains import (\n",
    "    StuffDocumentsChain,\n",
    "    LLMChain,\n",
    "    ReduceDocumentsChain,\n",
    "    MapReduceDocumentsChain,\n",
    ")\n",
    "from langchain_text_splitters.character import CharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_elasticsearch import ApproxRetrievalStrategy, ElasticsearchStore\n",
    "from langchain.schema import Document\n",
    "from langchain_core.runnables import RunnableLambda, Runnable, chain, RunnablePassthrough, RunnableBranch\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain.schema import StrOutputParser, Document\n",
    "from langchain_core.runnables.base import RunnableEach\n",
    "from unstructured.partition.auto import partition\n",
    "from unstructured.chunking.basic import chunk_elements\n",
    "\n",
    "from core_api.src.publisher_handler import FilePublisher\n",
    "from redbox.storage import ElasticsearchStorageHandler\n",
    "from redbox.models import File\n",
    "from redbox.models.settings import Settings\n",
    "from redbox.models.file import Metadata, UUID, PersistableModel\n",
    "from redbox.models.chat import ChatRequest, ChatResponse\n",
    "from redbox.storage import ElasticsearchStorageHandler\n",
    "from redbox.llm.prompts.core import _core_redbox_prompt\n",
    "from redbox.storage.storage_handler import BaseStorageHandler\n",
    "from redbox.models.file import Chunk, File\n",
    "from redbox.model_db import SentenceTransformerDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator_user_uuid=UUID('673f53f0-15e5-4ca1-be4b-41adcf602ab8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Settings(_env_file=\".env\")\n",
    "es_root_index = \"summarisation\"\n",
    "\n",
    "output_max_tokens=256\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts=[\n",
    "        {\n",
    "            \"host\": \"localhost\",\n",
    "            \"port\": env.elastic.port,\n",
    "            \"scheme\": env.elastic.scheme,\n",
    "        }\n",
    "    ],\n",
    "    basic_auth=(env.elastic.user, env.elastic.password),\n",
    ")\n",
    "if env.elastic.subscription_level == \"basic\":\n",
    "    strategy = ApproxRetrievalStrategy(hybrid=False)\n",
    "elif env.elastic.subscription_level in [\"platinum\", \"enterprise\"]:\n",
    "    strategy = ApproxRetrievalStrategy(hybrid=True)\n",
    "\n",
    "sentence_transformer_db = SentenceTransformerDB(env.embedding_model)\n",
    "\n",
    "vector_store = ElasticsearchStore(\n",
    "    es_connection=es,\n",
    "    index_name=\"redbox-data-chunk\",\n",
    "    embedding=env.embedding_model,\n",
    "    strategy=strategy,\n",
    "    vector_query_field=\"embedding\",\n",
    ")\n",
    "\n",
    "s3_client = boto3.client(\"s3\", endpoint_url=f\"http://{env.minio_host}:{env.minio_port}\", aws_access_key_id=env.aws_access_key, aws_secret_access_key=env.aws_secret_key)\n",
    "\n",
    "storage_handler = ElasticsearchStorageHandler(\n",
    "    es_client=es,\n",
    "    root_index=es_root_index\n",
    ")\n",
    "\n",
    "api_base=\"https://oai-i-dot-ai-playground-sweden.openai.azure.com/\"\n",
    "\n",
    "print(os.environ[\"AZURE_OPENAI_API_KEY\"])\n",
    "\n",
    "llm = ChatLiteLLM(\n",
    "    model=\"azure/gpt-35-turbo\",\n",
    "    api_base=api_base,\n",
    "    max_tokens=output_max_tokens\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Ingest Pipeline ###\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from mypy_boto3_s3.client import S3Client\n",
    "else:\n",
    "    S3Client = object\n",
    "\n",
    "@dataclass\n",
    "class LocalFile:\n",
    "    creator_user_uuid: UUID\n",
    "    filepath: Path\n",
    "\n",
    "\n",
    "def upload_file(\n",
    "        storage_handler: BaseStorageHandler, \n",
    "        s3: S3Client,\n",
    "        env: Settings\n",
    "    ):\n",
    "    @chain\n",
    "    def wrapped(local_file: LocalFile) -> File:\n",
    "        file_uuid = str(uuid4())\n",
    "        s3.put_object(Bucket=env.bucket_name, Key=str(file_uuid), Body=open(local_file.filepath, 'rb'))\n",
    "        file = File(uuid=file_uuid, creator_user_uuid=local_file.creator_user_uuid, key=file_uuid, bucket=env.bucket_name)\n",
    "        storage_handler.write_item(file)\n",
    "        return file\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "def file_chunker(env: Settings, s3_client: S3Client, max_chunk_size: int = 20000):\n",
    "    @chain\n",
    "    def wrapped(file: File) -> List[Chunk]:\n",
    "        authenticated_s3_url = s3_client.generate_presigned_url(\n",
    "            \"get_object\",\n",
    "            Params={\"Bucket\": file.bucket, \"Key\": file.key},\n",
    "            ExpiresIn=3600,\n",
    "        )\n",
    "        elements = partition(url=authenticated_s3_url, strategy=env.partition_strategy)\n",
    "        raw_chunks = chunk_elements(\n",
    "            elements, \n",
    "            new_after_n_chars=max_chunk_size, \n",
    "            max_characters=max_chunk_size+32\n",
    "        )\n",
    "        print(f\"Elements chunked\")\n",
    "        return [\n",
    "            Chunk(\n",
    "                parent_file_uuid=file.uuid,\n",
    "                index=i,\n",
    "                text=raw_chunk.text,\n",
    "                metadata=Metadata(\n",
    "                    parent_doc_uuid=file.uuid,\n",
    "                    page_number=raw_chunk.metadata.page_number,\n",
    "                    languages=raw_chunk.metadata.languages,\n",
    "                    link_texts=raw_chunk.metadata.link_texts,\n",
    "                    link_urls=raw_chunk.metadata.link_urls,\n",
    "                    links=raw_chunk.metadata.links,\n",
    "                ),\n",
    "                creator_user_uuid=file.creator_user_uuid,\n",
    "            )\n",
    "            for i, raw_chunk in enumerate(raw_chunks)\n",
    "        ]\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "def local_embedder(model: SentenceTransformerDB):\n",
    "    @chain\n",
    "    def wrapped(chunks: List[Chunk]) -> List[Chunk]:\n",
    "        print(f\"Starting Embedding\")\n",
    "        embedded_sentences = model.embed_sentences([c.text for c in chunks])\n",
    "        for i, c in enumerate(chunks):\n",
    "            c.embedding = embedded_sentences.data[i].embedding\n",
    "        return chunks\n",
    "    return wrapped\n",
    "\n",
    "def chunk_writer(storage_handler: BaseStorageHandler):\n",
    "    @chain\n",
    "    def wrapped(chunks: List[Chunk]) -> UUID:\n",
    "        print(f\"Writing Chunks\")\n",
    "        storage_handler.write_items(chunks)\n",
    "        return chunks[0].parent_file_uuid\n",
    "    return wrapped\n",
    "\n",
    "def summarisation_ingest_chain(n=20000):\n",
    "    chain =(\n",
    "        upload_file(storage_handler, s3_client, env)\n",
    "        | file_chunker(env, s3_client, max_chunk_size=4000)\n",
    "        | local_embedder(sentence_transformer_db)\n",
    "        | chunk_writer(storage_handler)\n",
    "    )\n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Execution Ingest ###\n",
    "\n",
    "es.options(ignore_status=[400,404]).indices.delete(index=f\"{es_root_index}-file\")\n",
    "es.options(ignore_status=[400,404]).indices.delete(index=f\"{es_root_index}-chunk\")\n",
    "\n",
    "summarisation_ingest = summarisation_ingest_chain()\n",
    "\n",
    "ingest_result = summarisation_ingest.invoke(\n",
    "    LocalFile(\n",
    "        filepath=Path(\"../data/TS_Rules_Deluxe.pdf\"),\n",
    "        creator_user_uuid=creator_user_uuid\n",
    "    )\n",
    ")\n",
    "\n",
    "file_uuid = ingest_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Summarisation Pipeline ###\n",
    "\n",
    "def document_reader(storage_handler: BaseStorageHandler, user_uuid):\n",
    "    @chain\n",
    "    def wrapped(parent_file_uuid):\n",
    "        chunks = storage_handler.get_file_chunks(\n",
    "            parent_file_uuid=parent_file_uuid,\n",
    "            user_uuid=user_uuid\n",
    "        )\n",
    "        return [\n",
    "            Document(page_content=chunk.text, metadata={\"source\": \"local\"})\n",
    "            for chunk in chunks\n",
    "        ]\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "\n",
    "max_prompt_size = 4096 - output_max_tokens - 32\n",
    "\n",
    "@chain\n",
    "def summarise(file_uuid):\n",
    "    docs = document_reader(storage_handler, creator_user_uuid).invoke(file_uuid)\n",
    "    mapreduce_loops = 0\n",
    "    while mapreduce_loops < 3:\n",
    "        summaries = (\n",
    "            ChatPromptTemplate.from_template(\"Summarize this content: {context}\")\n",
    "            | llm\n",
    "        ).batch(\n",
    "            docs, \n",
    "            config=RunnableConfig(\n",
    "                max_concurrency=64\n",
    "            )\n",
    "        )\n",
    "        prompt_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "            encoding_name=\"cl100k_base\", chunk_size=max_prompt_size, chunk_overlap=0\n",
    "        )\n",
    "\n",
    "        def combine_summaries(summaries):\n",
    "            return \" ; \".join([s.content for s in summaries[:24]])\n",
    "\n",
    "        combined = combine_summaries(summaries)\n",
    "        summarise_prompt = ChatPromptTemplate.from_template(\"Combine these summaries: {docs}\").invoke(combined)\n",
    "        if (len(prompt_splitter.split_text(str(summarise_prompt))) == 1):\n",
    "            # Stop summarising the summaries we can go to final summary\n",
    "            break\n",
    "        else:\n",
    "            # We can't do a summary of all docs due to length so combine them into two docs and summarise again\n",
    "            number_summaries = int(len(summaries)/2)\n",
    "            docs = [\n",
    "                summarise_prompt.invoke(combine_summaries(summaries[:number_summaries])),\n",
    "                summarise_prompt.invoke(combine_summaries(summaries[number_summaries:]))\n",
    "            ]\n",
    "            mapreduce_loops += 1\n",
    "    else:\n",
    "        # Panic because we're looping a long time to get this down to a reasonable size?\n",
    "        print(\"Too many loops\")\n",
    "    result = llm.invoke(summarise_prompt)\n",
    "    return StrOutputParser().invoke(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_uuid=\"5a5ff2df-dff8-46d8-af43-12f7c99db429\"\n",
    "answer = summarise.invoke(file_uuid)\n",
    "\n",
    "print(f\"[{file_uuid}] {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redbox-ZGz5Jzj1-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

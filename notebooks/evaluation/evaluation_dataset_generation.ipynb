{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create evaluation dataset for Redbox RAG chat  <a class=\"anchor\" id=\"title\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Redbox RAG chat on one stable, numbered version of these data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before running this notebook**\n",
    "\n",
    "Set the version of the evaluation dataset you are creating **[HERE](#setversion)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a class=\"anchor\" id=\"toc\"></a>\n",
    "* [Overview](#overview)\n",
    "* [Set version of the evaluation dataset](#setversion)\n",
    "* [Select files for creating evaluation dataset](#files)\n",
    "* [Imports](#imports)\n",
    "* [Generate Evaluation Dataset](#ragas)\n",
    "* [Save Evaluation Dataset](#save)\n",
    "* [Troubleshooting](#troubleshooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview <a class=\"anchor\" id=\"overview\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is really important to version the evaluations we are doing, including the input data used to generate evaluation datasets.\n",
    "\n",
    "This notebook uses the files you select in combination with the RAGAS framework to generate synthetic data. Two different LLMs are used, one for the 'generator' and one for the 'critic'.\n",
    "\n",
    "Please be aware the generating synthetic data will incur LLM API costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a troubleshooting section at the end of this notebook [Troubleshooting](#troubleshooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Redbox RAG chat on one stable, numbered version of these data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the version of the evaluation dataset you will be creating in this notebook in the cell below**  <a class=\"anchor\" id=\"setversion\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_VERSION = \"0.1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to set up the required folder structure (it will not overwrite folders and files if they already exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parents[1]\n",
    "EVALUATION_DIR = ROOT / \"notebooks/evaluation\"\n",
    "\n",
    "V_ROOT = EVALUATION_DIR / f\"data/{DATA_VERSION}\"\n",
    "V_RAW = V_ROOT / \"raw\"\n",
    "V_SYNTHETIC = V_ROOT / \"synthetic\"\n",
    "V_CHUNKS = V_ROOT / \"chunks\"\n",
    "V_RESULTS = V_ROOT / \"results\"\n",
    "V_EMBEDDINGS = V_ROOT / \"embeddings\"\n",
    "\n",
    "V_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "V_RAW.mkdir(parents=True, exist_ok=True)\n",
    "V_SYNTHETIC.mkdir(parents=True, exist_ok=True)\n",
    "V_CHUNKS.mkdir(parents=True, exist_ok=True)\n",
    "V_RESULTS.mkdir(parents=True, exist_ok=True)\n",
    "V_EMBEDDINGS.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select files that you will use to generate versioned evaluation dataset   <a class=\"anchor\" id=\"files\"></a>\n",
    "\n",
    "Now copy all the files you want to use to generate **THIS VERSION** of the evaluation dataset into `notebooks/evaluation/data/{DATA_VERSION}/raw/`\n",
    "\n",
    "Also upload these files to shared Google Drive and the corresponding version number/location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports <a id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import typing as t\n",
    "import json\n",
    "import jsonlines\n",
    "import pickle\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetically generate evaluation dataset <a class=\"anchor\" id=\"ragas\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAGAS generating a synthetic test set detailed [HERE](https://docs.ragas.io/en/stable/getstarted/testset_generation.html). Perhaps not as SOTA as DeepEval (validate!), but it creates `input` AND `expected_output` for us. \n",
    "\n",
    "So we are not generating input questions based on our chunking strategy, however, we are using the same files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Takes about 4 minutes for 4 docs. Consider Langchain `unstructured`\n",
    "loader = DirectoryLoader(V_RAW)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Langchain documents for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_docs_to_jsonl(documents: t.Iterable[Document], file_path: str) -> None:\n",
    "    with jsonlines.open(file_path, mode=\"w\") as writer:\n",
    "        for doc in documents:\n",
    "            writer.write(doc.dict())\n",
    "\n",
    "\n",
    "def load_docs_from_jsonl(file_path) -> t.Iterable[Document]:\n",
    "    documents = []\n",
    "    with jsonlines.open(file_path, mode=\"r\") as reader:\n",
    "        for doc in reader:\n",
    "            documents.append(Document(**doc))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_docs_to_jsonl(documents, V_CHUNKS / \"documents.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGAS generator with openai models\n",
    "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\") # to match core-api\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o\") # cheaper model with similar performance\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457f9d93664d4f93950634b87560bc3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb1ad3e0e814f4fa2525b309a9b90b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate testset\n",
    "testset = generator.generate_with_langchain_docs(documents, test_size=10, distributions={simple: 0.4, reasoning: 0.3, multi_context: 0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save RAGAS generated testset <a class=\"anchor\" id=\"save\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{V_SYNTHETIC}/ragas_testset.pkl', 'wb') as f:\n",
    "    pickle.dump(testset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataframe into a DeepEval compatible CSV & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df = testset.to_pandas()\n",
    "\n",
    "# Rename the columns\n",
    "new_column_names = {\n",
    "    'question': 'input',\n",
    "    'contexts': 'context',\n",
    "    'ground_truth': 'expected_output',\n",
    "    # Add more column names here\n",
    "}\n",
    "\n",
    "testset_df_renamed = testset_df.rename(columns=new_column_names)\n",
    "\n",
    "#  DeepEval dataset format requires an 'actual_output' column\n",
    "testset_df_renamed['actual_output'] = ''\n",
    "testset_df_renamed = testset_df_renamed.drop(['evolution_type', 'metadata', 'episode_done'], axis=1)\n",
    "\n",
    "# Convert all columns to string & drop NaN - otherwise DeepEval will throw an Pydantic validation error\n",
    "testset_df_renamed = testset_df_renamed.astype(str)\n",
    "testset_df_renamed = testset_df_renamed.dropna()\n",
    "\n",
    "# save as CSV\n",
    "testset_df_renamed.to_csv(f'{V_SYNTHETIC}/ragas_synthetic_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) View top 5 rows of synthetically generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df_renamed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-embed the documents for other users\n",
    "\n",
    "Embeddings take a while. Here we show how to compute and save them for other users.\n",
    "\n",
    "For now we use the chunking strategy from `worker/`, and embed with any models we choose.\n",
    "\n",
    "Ensure the necessary services are running with `make eval_backend`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redbox.model_db import SentenceTransformerDB\n",
    "from redbox.models import Settings\n",
    "from redbox.parsing import chunk_file\n",
    "from redbox.storage.elasticsearch import ElasticsearchStorageHandler\n",
    "from redbox.models.file import File\n",
    "\n",
    "from minio import Minio\n",
    "from uuid import UUID\n",
    "\n",
    "env = Settings()\n",
    "\n",
    "minio = Minio(\n",
    "    endpoint=f\"localhost:{env.minio_port}\",\n",
    "    access_key=env.aws_access_key,\n",
    "    secret_key=env.aws_access_key,\n",
    ")\n",
    "\n",
    "USER_UUID = UUID(\"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'redbox-storage-dev'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.bucket_name\n",
    "\n",
    "from minio import Minio\n",
    "\n",
    "client = Minio(\"play.min.io\",\n",
    "    access_key=\"Q3AM3UQ867SPQQA43P2F\",\n",
    "    secret_key=\"zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CS013b_Energy stats monthly brief september 2022.pdf'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(V_RAW.glob(\"*.*\")).name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PosixPath' object has no attribute 'bucket'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m V_RAW\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.*\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[43mchunk_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chunks))\n",
      "File \u001b[0;32m~/DS/redbox-copilot/redbox/parsing/file_chunker.py:52\u001b[0m, in \u001b[0;36mchunk_file\u001b[0;34m(file, embedding_model)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_file\u001b[39m(\n\u001b[1;32m     37\u001b[0m     file: File,\n\u001b[1;32m     38\u001b[0m     embedding_model: SentenceTransformer \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     39\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[Chunk]:\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m        file (File): The file to read, analyse layout and chunk.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m        Sequence[Chunk]: The chunks generated from the given file.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[43mother_chunker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embedding_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m cluster_chunks(chunks, embedding_model\u001b[38;5;241m=\u001b[39membedding_model)\n",
      "File \u001b[0;32m~/DS/redbox-copilot/redbox/parsing/chunkers.py:25\u001b[0m, in \u001b[0;36mother_chunker\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mother_chunker\u001b[39m(file: File) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[Chunk]:\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The default unstructured chunker for Redbox. This chunker uses the unstructured partitioner and title chunker\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    to split a file into chunks.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m        list[Chunk]: A list of chunks that have been created from the file.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     authenticated_s3_url \u001b[38;5;241m=\u001b[39m s3_client\u001b[38;5;241m.\u001b[39mgenerate_presigned_url(\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_object\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m---> 25\u001b[0m         Params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBucket\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbucket\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey\u001b[39m\u001b[38;5;124m\"\u001b[39m: file\u001b[38;5;241m.\u001b[39mkey},\n\u001b[1;32m     26\u001b[0m         ExpiresIn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3600\u001b[39m,\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     29\u001b[0m     elements \u001b[38;5;241m=\u001b[39m partition(url\u001b[38;5;241m=\u001b[39mauthenticated_s3_url, strategy\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mpartition_strategy)\n\u001b[1;32m     30\u001b[0m     raw_chunks \u001b[38;5;241m=\u001b[39m chunk_by_title(elements\u001b[38;5;241m=\u001b[39melements)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute 'bucket'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for file_path in V_RAW.glob(\"*.*\"):\n",
    "    key = f\"{DATA_VERSION}/{file.name}\"\n",
    "    file = File(key=key, bucket=env.bucket_name, creator_user_uuid=)\n",
    "    minio.fput_object(\n",
    "        bucket_name=env.bucket_name,\n",
    "        object_name=key,\n",
    "        file_path=file,\n",
    "    )\n",
    "    chunks = chunk_file(file=file)\n",
    "    print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_handler = ElasticsearchStorageHandler(es_client=es, root_index=env.elastic_root_index)\n",
    "model = SentenceTransformerDB(env.embedding_model)\n",
    "model.embed_sentences([chunk.text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting <a class=\"anchor\" id=\"troubleshooting\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain DirectoryLoader Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run into a poppler path error and poppler is installed and can be access from your virtual environment (by running `pdfinfo -v`), then close notebook and restart the Jupyter server from the terminal where the path is correctly set (by running `code notebooks/evaluation/evaluation_dataset_generation.ipynb`) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAGAS synthetically generated evaluation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found some rows of synthetically generated evaluation data from using the RAGAS framework, includes some NaN and/or not str type, which results in an error for DeepEval metrics, as these data fail Pydantic validation.\n",
    "\n",
    "To avoid this, ensure you turn RAGAS synthetically generated evaluation data to type str and remove rows of data with NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepEval framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, this notebook only loads the evaluation dataset into DeepEval from a CSV. There is a JSON import option that we are not using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redbox-MiicHf1r-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

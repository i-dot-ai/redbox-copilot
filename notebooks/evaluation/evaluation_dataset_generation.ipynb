{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create evaluation dataset for Redbox RAG chat  <a class=\"anchor\" id=\"title\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Redbox RAG chat on one stable, numbered version of these data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before running this notebook**\n",
    "\n",
    "Set the version of the evaluation dataset you are creating **[HERE](#setversion)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a class=\"anchor\" id=\"toc\"></a>\n",
    "* [Overview](#overview)\n",
    "* [Set version of the evaluation dataset](#setversion)\n",
    "* [Select files for creating evaluation dataset](#files)\n",
    "* [Imports](#imports)\n",
    "* [Generate Evaluation Dataset](#ragas)\n",
    "* [Save Evaluation Dataset](#save)\n",
    "* [Troubleshooting](#troubleshooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview <a class=\"anchor\" id=\"overview\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is really important to version the evaluations we are doing, including the input data used to generate evaluation datasets.\n",
    "\n",
    "This notebook uses the files you select in combination with the RAGAS framework to generate synthetic data. Two different LLMs are used, one for the 'generator' and one for the 'critic'.\n",
    "\n",
    "Please be aware the generating synthetic data will incur LLM API costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a troubleshooting section at the end of this notebook [Troubleshooting](#troubleshooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Redbox RAG chat on one stable, numbered version of these data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the version of the evaluation dataset you will be creating in this notebook in the cell below**  <a class=\"anchor\" id=\"setversion\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_VERSION = \"0.1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to set up the required folder structure (it will not overwrite folders and files if they already exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parents[1]\n",
    "EVALUATION_DIR = ROOT / \"notebooks/evaluation\"\n",
    "\n",
    "V_ROOT = EVALUATION_DIR / f\"data/{DATA_VERSION}\"\n",
    "V_RAW = V_ROOT / \"raw\"\n",
    "V_SYNTHETIC = V_ROOT / \"synthetic\"\n",
    "V_CHUNKS = V_ROOT / \"chunks\"\n",
    "V_RESULTS = V_ROOT / \"results\"\n",
    "V_EMBEDDINGS = V_ROOT / \"embeddings\"\n",
    "\n",
    "V_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "V_RAW.mkdir(parents=True, exist_ok=True)\n",
    "V_SYNTHETIC.mkdir(parents=True, exist_ok=True)\n",
    "V_CHUNKS.mkdir(parents=True, exist_ok=True)\n",
    "V_RESULTS.mkdir(parents=True, exist_ok=True)\n",
    "V_EMBEDDINGS.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's helpful for all calls to share a dummy user. Set that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import UUID\n",
    "\n",
    "USER_UUID = UUID(\"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select files that you will use to generate versioned evaluation dataset   <a class=\"anchor\" id=\"files\"></a>\n",
    "\n",
    "Now copy all the files you want to use to generate **THIS VERSION** of the evaluation dataset into `notebooks/evaluation/data/{DATA_VERSION}/raw/`\n",
    "\n",
    "Also upload these files to shared Google Drive and the corresponding version number/location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports <a id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import typing as t\n",
    "import json\n",
    "import jsonlines\n",
    "import pickle\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Connecting to self managed Elasticsearch\n",
      "INFO:root:Elasticsearch host = localhost\n",
      "INFO:elastic_transport.transport:HEAD http://localhost:9200/ [status:200 duration:0.004s]\n"
     ]
    }
   ],
   "source": [
    "from redbox.model_db import SentenceTransformerDB\n",
    "from redbox.models import Settings\n",
    "from redbox.parsing import chunk_file\n",
    "from redbox.models.file import File, Chunk\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "from mypy_boto3_s3.client import S3Client\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "ENV = Settings()\n",
    "ENV.minio_host = \"localhost\"\n",
    "ENV.elastic.host = \"localhost\"\n",
    "\n",
    "S3_CLIENT = ENV.s3_client()\n",
    "ES_CLIENT = ENV.elasticsearch_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetically generate evaluation dataset <a class=\"anchor\" id=\"ragas\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAGAS generating a synthetic test set detailed [HERE](https://docs.ragas.io/en/stable/getstarted/testset_generation.html). Perhaps not as SOTA as DeepEval (validate!), but it creates `input` AND `expected_output` for us. \n",
    "\n",
    "So we are not generating input questions based on our chunking strategy, however, we are using the same files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Takes about 4 minutes for 4 docs. Consider Langchain `unstructured`\n",
    "loader = DirectoryLoader(V_RAW)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Langchain documents for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_docs_to_jsonl(documents: t.Iterable[Document], file_path: str) -> None:\n",
    "    with jsonlines.open(file_path, mode=\"w\") as writer:\n",
    "        for doc in documents:\n",
    "            writer.write(doc.dict())\n",
    "\n",
    "\n",
    "def load_docs_from_jsonl(file_path) -> t.Iterable[Document]:\n",
    "    documents = []\n",
    "    with jsonlines.open(file_path, mode=\"r\") as reader:\n",
    "        for doc in reader:\n",
    "            documents.append(Document(**doc))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_docs_to_jsonl(documents, V_CHUNKS / \"documents.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGAS generator with openai models\n",
    "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\") # to match core-api\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o\") # cheaper model with similar performance\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457f9d93664d4f93950634b87560bc3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb1ad3e0e814f4fa2525b309a9b90b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate testset\n",
    "testset = generator.generate_with_langchain_docs(documents, test_size=10, distributions={simple: 0.4, reasoning: 0.3, multi_context: 0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save RAGAS generated testset <a class=\"anchor\" id=\"save\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{V_SYNTHETIC}/ragas_testset.pkl', 'wb') as f:\n",
    "    pickle.dump(testset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataframe into a DeepEval compatible CSV & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df = testset.to_pandas()\n",
    "\n",
    "# Rename the columns\n",
    "new_column_names = {\n",
    "    'question': 'input',\n",
    "    'contexts': 'context',\n",
    "    'ground_truth': 'expected_output',\n",
    "    # Add more column names here\n",
    "}\n",
    "\n",
    "testset_df_renamed = testset_df.rename(columns=new_column_names)\n",
    "\n",
    "#  DeepEval dataset format requires an 'actual_output' column\n",
    "testset_df_renamed['actual_output'] = ''\n",
    "testset_df_renamed = testset_df_renamed.drop(['evolution_type', 'metadata', 'episode_done'], axis=1)\n",
    "\n",
    "# Convert all columns to string & drop NaN - otherwise DeepEval will throw an Pydantic validation error\n",
    "testset_df_renamed = testset_df_renamed.astype(str)\n",
    "testset_df_renamed = testset_df_renamed.dropna()\n",
    "\n",
    "# save as CSV\n",
    "testset_df_renamed.to_csv(f'{V_SYNTHETIC}/ragas_synthetic_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) View top 5 rows of synthetically generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df_renamed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-embed the documents for other users\n",
    "\n",
    "Embeddings take a while. Here we show how to compute and save them for other users.\n",
    "\n",
    "For now we use the chunking strategy from `worker/`, and embed with any models we choose.\n",
    "\n",
    "Ensure the necessary services are running with `make eval_backend`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunks_to_jsonl(chunks: t.Iterable[Chunk], file_path: Path) -> None:\n",
    "    with jsonlines.open(file_path, mode=\"w\") as writer:\n",
    "        for chunk in chunks:\n",
    "            writer.write(chunk.model_dump_json())\n",
    "\n",
    "\n",
    "def load_chunks_from_jsonl_to_index(file_path: Path, es_client: Elasticsearch, index: str) -> None:\n",
    "    with jsonlines.open(file_path, mode=\"r\") as reader:\n",
    "        for chunk_raw in reader:\n",
    "            chunk = json.loads(chunk_raw)\n",
    "            es_client.index(\n",
    "                index=index,\n",
    "                id=chunk[\"uuid\"],\n",
    "                body=chunk,\n",
    "            )\n",
    "\n",
    "\n",
    "def embed_file(\n",
    "    file_path: Path, \n",
    "    data_version: str, \n",
    "    bucket_name: str, \n",
    "    model: SentenceTransformerDB, \n",
    "    user_uuid: UUID = USER_UUID, \n",
    "    s3_client: S3Client = S3_CLIENT\n",
    ") -> list[Chunk]:\n",
    "    key = f\"{data_version}/{file_path.name}\"\n",
    "    file = File(key=key, bucket=bucket_name, creator_user_uuid=user_uuid)\n",
    "    \n",
    "    # Upload to bucket\n",
    "    with open(file_path, 'rb') as f:\n",
    "        s3_client.upload_fileobj(f, bucket_name, key)\n",
    "    \n",
    "    # Chunk\n",
    "    chunks = chunk_file(file=file, s3_client=s3_client)\n",
    "\n",
    "    # Embed\n",
    "    embeddings = [\n",
    "        embedding.embedding \n",
    "        for embedding \n",
    "        in model.embed_sentences([chunk.text for chunk in chunks]).data\n",
    "    ]\n",
    "\n",
    "    # Merge\n",
    "    chunks_embedded = []\n",
    "    for chunk, embedding in zip(chunks, embeddings, strict=True):\n",
    "        chunk_embedded = Chunk(\n",
    "            uuid=chunk.uuid, \n",
    "            created_datetime=chunk.created_datetime, \n",
    "            creator_user_uuid=chunk.creator_user_uuid, \n",
    "            parent_file_uuid=chunk.parent_file_uuid, \n",
    "            index=chunk.index, \n",
    "            text=chunk.text, \n",
    "            metadata=chunk.metadata, \n",
    "            embedding=embedding\n",
    "        )\n",
    "        chunks_embedded.append(chunk_embedded)\n",
    "    \n",
    "    return chunks_embedded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe318e99b334882a6f78f0a6f301c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df8ccf9af8445c19711ac386518d67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL = ENV.embedding_model\n",
    "EMBEDDING_MODEL = f\"{DATA_VERSION}-{MODEL.embedding_model_name}\".lower()\n",
    "\n",
    "all_embedded_files = []\n",
    "\n",
    "for file_path in V_RAW.glob(\"*.*\"):\n",
    "    file_chunks_embedded = embed_file(\n",
    "        file_path=file_path,\n",
    "        data_version=DATA_VERSION,\n",
    "        bucket_name=ENV.bucket_name,\n",
    "        model=SentenceTransformerDB(model_name=EMBEDDING_MODEL)\n",
    "    )\n",
    "    all_embedded_files += file_chunks_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chunks_to_jsonl(\n",
    "    chunks=all_embedded_files, \n",
    "    file_path=V_EMBEDDINGS / f\"{model.embedding_model_name}.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.0-all-minilm-l6-v2'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{DATA_VERSION}-{model.embedding_model_name}\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/4b8c64a9-3e9f-4aee-9454-6b36bf76bee5 [status:201 duration:0.507s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/c2ebedbd-fbf1-4f76-846b-374a22d7c63d [status:201 duration:0.019s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/103e0e9d-2a32-4e85-82b1-bc7d6ca918b3 [status:201 duration:0.015s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/66a0d377-fb51-45b1-a2ca-7dc868324704 [status:201 duration:0.014s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/68fbc7bc-bb31-40d0-b43a-da8f72151fcd [status:201 duration:0.011s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/b1f812f6-1188-4a63-afd2-64ce2a1bc947 [status:201 duration:0.011s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/97fb8f43-dc26-4e9a-a13b-2a7eabe4fabe [status:201 duration:0.010s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/4e72dc12-7968-4396-aaed-9114c94a0396 [status:201 duration:0.011s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/9501a2fe-fcf1-4b5c-83ca-4b112918661c [status:201 duration:0.010s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/d6fa4e2f-9af9-4aff-8f48-9de24b0fdd7a [status:201 duration:0.027s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/821d81b8-56ab-408d-9441-c0e7a7adece7 [status:201 duration:0.013s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/12f8b110-8092-4919-b2b1-c148e831416a [status:201 duration:0.012s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/d12f53da-e456-4b20-a6b2-456e29113046 [status:201 duration:0.012s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/ee5e0ea2-4a7e-44a2-b130-13ab4f659bb8 [status:201 duration:0.011s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/ced011f5-d51e-4dd7-b398-62fae1ea1af4 [status:201 duration:0.012s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/bacb0cc5-1166-4816-8e22-1184240c33ce [status:201 duration:0.017s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/108f4a0e-3c1a-4462-adef-354a0b800d41 [status:201 duration:0.009s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/5e4eec8d-d056-4bf6-b854-6e5dc5de816b [status:201 duration:0.013s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/be1dc32b-5416-4490-baf0-547bc5da4127 [status:201 duration:0.009s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/489bb0f9-d9ab-4b77-a523-5e4153639eab [status:201 duration:0.016s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/c9483e6b-52fd-4213-87cf-dc2ead49823c [status:201 duration:0.009s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/dc58831a-5d0f-45d0-8028-1aa3cf9f7388 [status:201 duration:0.012s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/9f09e5bf-8561-4647-840c-8a37690295ce [status:201 duration:0.014s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/bb6bb71b-14ff-48bc-bcdf-472fb195681a [status:201 duration:0.014s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/ce4c65f6-a5a7-45a0-a0a9-1222c78b7c2a [status:201 duration:0.014s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/d1f643aa-1d37-4078-bb37-e62d8404a096 [status:201 duration:0.014s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/7cddbb0d-84e7-4204-a754-4a968a2a0e96 [status:201 duration:0.017s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/596a197a-ee7c-4479-9acf-3ed7a8ed78a4 [status:201 duration:0.014s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/0e2ff4b9-7aa0-4211-86a8-681275cdd4f2 [status:201 duration:0.013s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/27add82e-5473-4066-b331-59b60d95184b [status:201 duration:0.021s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/0525ba46-7907-40e4-a9c9-e0069c2de116 [status:201 duration:0.012s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/6418df98-915b-44d5-98b8-e373878d45eb [status:201 duration:0.011s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/a875dc71-0bcb-48a5-896b-52d1203310ba [status:201 duration:0.010s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/9d68d1b3-e7a5-441e-9bfb-87305b86ca36 [status:201 duration:0.013s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/5a989c3e-5b31-4a7e-88d3-edaec3f49f91 [status:201 duration:0.008s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/c3cdbbcd-f7ab-4ca4-9bb5-915630acd3e7 [status:201 duration:0.010s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/ca99e1cf-0c31-44b9-a384-e37b84e8912a [status:201 duration:0.008s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/b0179509-eb62-4362-a714-98ecb954ad4f [status:201 duration:0.009s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/719fb996-4eb9-4d2a-a378-2e3f82f111a5 [status:201 duration:0.009s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/e88d808c-0497-4af4-94c5-6d71a29b9932 [status:201 duration:0.012s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/029fa3bd-6623-4ef9-9acf-8efc9ab2b988 [status:201 duration:0.011s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/9029e8b9-aba1-459a-b9d2-18d12366a503 [status:201 duration:0.012s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/8bcdd0e7-02e5-4216-a4e0-0bee0e1e764e [status:201 duration:0.009s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/553c7512-0c30-41de-9dd8-30a1c1d38b1b [status:201 duration:0.007s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/7e17fa20-5fd4-447d-acd1-62e5179dcd27 [status:201 duration:0.008s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/b536358c-5bd5-447f-9709-df7eaff429b7 [status:201 duration:0.007s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/55614052-6383-4ac0-b0d5-7ad75f72b12c [status:201 duration:0.012s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/7d8b26eb-c982-42fc-bb68-540ef172a449 [status:201 duration:0.009s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/3bcd6744-4a20-45b6-88e1-0c953d98a1e5 [status:201 duration:0.011s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/d560dc14-ac58-4fb2-b359-ab880885b660 [status:201 duration:0.011s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/44f8cb7e-c1e4-4d46-827e-7c14e03faf08 [status:201 duration:0.008s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/c40ea327-9a07-42b1-85f8-2af235db80fb [status:201 duration:0.011s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/8a71d5ac-fb90-4021-aed6-ec104b7e3370 [status:201 duration:0.008s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/e4d1099d-2462-46b5-9382-f01f1d40e401 [status:201 duration:0.010s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/0acb4465-d8ef-434c-ace4-ce9848d06282 [status:201 duration:0.010s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/88c1f42d-edcc-4db5-acb3-4be5fb5d348d [status:201 duration:0.018s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/773c8bd7-6fdc-4344-ad1c-f4d30a05f2ef [status:201 duration:0.008s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/29fb1a94-32b3-46cf-b7cf-1d27e4f397da [status:201 duration:0.009s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/472d9379-e370-4ffe-986f-a0b346ea8193 [status:201 duration:0.011s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/ef093d6c-c011-4cf2-ae68-78025c0780e8 [status:201 duration:0.010s]\n",
      "INFO:elastic_transport.transport:PUT http://localhost:9200/0.1.0-all-minilm-l6-v2/_doc/f09e2fc4-d8c3-4940-b063-2e6045f6681f [status:201 duration:0.011s]\n"
     ]
    }
   ],
   "source": [
    "load_chunks_from_jsonl_to_index(\n",
    "    file_path=V_EMBEDDINGS / f\"{model.embedding_model_name}.jsonl\",\n",
    "    es_client=ES_CLIENT,\n",
    "    index=f\"{DATA_VERSION}-{model.embedding_model_name}\".lower()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting <a class=\"anchor\" id=\"troubleshooting\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain DirectoryLoader Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run into a poppler path error and poppler is installed and can be access from your virtual environment (by running `pdfinfo -v`), then close notebook and restart the Jupyter server from the terminal where the path is correctly set (by running `code notebooks/evaluation/evaluation_dataset_generation.ipynb`) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAGAS synthetically generated evaluation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found some rows of synthetically generated evaluation data from using the RAGAS framework, includes some NaN and/or not str type, which results in an error for DeepEval metrics, as these data fail Pydantic validation.\n",
    "\n",
    "To avoid this, ensure you turn RAGAS synthetically generated evaluation data to type str and remove rows of data with NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepEval framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, this notebook only loads the evaluation dataset into DeepEval from a CSV. There is a JSON import option that we are not using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redbox-MiicHf1r-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

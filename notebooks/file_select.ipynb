{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "from pathlib import Path\n",
                "import os\n",
                "import logging\n",
                "import sys\n",
                "from uuid import UUID\n",
                "\n",
                "from langchain_community.chat_models import ChatLiteLLM\n",
                "from langchain_community.embeddings import AzureOpenAIEmbeddings\n",
                "from langchain.schema import SystemMessage\n",
                "from langchain_elasticsearch.vectorstores import ElasticsearchStore, ApproxRetrievalStrategy\n",
                "from elasticsearch import Elasticsearch\n",
                "\n",
                "from redbox.models import Settings, File\n",
                "from redbox.models.settings import ElasticLocalSettings\n",
                "from redbox.storage import ElasticsearchStorageHandler\n",
                "\n",
                "from pydantic import BaseModel\n",
                "\n",
                "from dotenv import find_dotenv, load_dotenv\n",
                "import json\n",
                "from humanize import naturaltime\n",
                "\n",
                "ROOT = Path().resolve().parent\n",
                "sys.path.append(str(ROOT))\n",
                "\n",
                "_ = load_dotenv(find_dotenv(ROOT / '.env'))\n",
                "\n",
                "logging.basicConfig(steam=sys.stdout, level=logging.INFO)\n",
                "log = logging.getLogger()\n",
                "\n",
                "env = Settings(\n",
                "    _env_file=(ROOT / '.env'),\n",
                "    minio_host=\"localhost\", \n",
                "    object_store=\"minio\",\n",
                "    elastic=ElasticLocalSettings(host=\"localhost\"),\n",
                ")\n",
                "\n",
                "es = Elasticsearch(\n",
                "    hosts=[\n",
                "        {\n",
                "            \"host\": \"localhost\",\n",
                "            \"port\": env.elastic.port,\n",
                "            \"scheme\": env.elastic.scheme,\n",
                "        }\n",
                "    ],\n",
                "    basic_auth=(env.elastic.user, env.elastic.password),\n",
                ")\n",
                "\n",
                "if env.elastic.subscription_level == \"basic\":\n",
                "    strategy = ApproxRetrievalStrategy(hybrid=False)\n",
                "elif env.elastic.subscription_level in [\"platinum\", \"enterprise\"]:\n",
                "    strategy = ApproxRetrievalStrategy(hybrid=True)\n",
                "\n",
                "embedding_model = AzureOpenAIEmbeddings(\n",
                "    model=env.azure_embedding_model,\n",
                "    azure_endpoint=env.azure_openai_endpoint,\n",
                "    openai_api_base=None,\n",
                "    openai_api_key=env.azure_openai_api_key,\n",
                "    openai_api_version=env.openai_api_version,\n",
                "    azure_ad_token=None,\n",
                "    openai_organization=None,\n",
                "    chunk_size=2048\n",
                ")\n",
                "\n",
                "\n",
                "vector_store = ElasticsearchStore(\n",
                "    es_connection=es,\n",
                "    index_name=\"redbox-data-integration-chunk\",\n",
                "    embedding=embedding_model,\n",
                "    strategy=strategy,\n",
                "    vector_query_field=\"azure_embedding\",\n",
                ")\n",
                "\n",
                "# See core_api.dependecies for details on this hack\n",
                "os.environ[\"AZURE_API_VERSION\"] = env.openai_api_version\n",
                "\n",
                "\n",
                "\n",
                "llm = ChatLiteLLM(\n",
                "    model=env.azure_openai_model,\n",
                "    streaming=True,\n",
                "    azure_key=env.azure_openai_api_key,\n",
                "    api_base=env.azure_openai_endpoint,\n",
                "    max_tokens=1_024,\n",
                ")\n",
                "\n",
                "storage_handler = ElasticsearchStorageHandler(es_client=es, root_index=env.elastic_root_index+\"-integration\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quick test to check if the model is working\n",
                "\n",
                "user_query = \"What can we do to improve school inspections?\"\n",
                "\n",
                "\n",
                "vector_store.similarity_search(query=user_query, k=10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Three approaches to develop:\n",
                "- `LIST`: List available files and have LLM select\n",
                "- `LIST_SUMMARY`: List available files with a brief summary description and have LLM select\n",
                "- `PARENT_DOC_RETRIEVER`: Do a similarity search on chunks and retrieve the most relevant parent files\n",
                "  - `DENSITY_PARENT_DOC_RETRIEVER`: Perform a density analysis to determine which files have the most relevant content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LIST approach\n",
                "\n",
                "USER_UUID = \"5b57d820-1b6b-41db-b265-ca046e7a9674\"\n",
                "\n",
                "def simplify_file(file: File) -> dict:\n",
                "    \"\"\"\n",
                "    Convert a File object into a simple dictionary that can be used in prompt by the LLM\n",
                "\n",
                "    Args:\n",
                "        file (File): The file object to simplify\n",
                "\n",
                "    Returns:\n",
                "        dict: A simplified dictionary representation of the file\n",
                "    \"\"\"\n",
                "\n",
                "    simple_file = file.model_dump()\n",
                "\n",
                "    # Remove fields that are not needed by LLM\n",
                "    del simple_file[\"model_type\"]\n",
                "    del simple_file[\"creator_user_uuid\"]\n",
                "    del simple_file[\"ingest_status\"]\n",
                "    del simple_file[\"bucket\"]\n",
                "\n",
                "    # Add a relative datetime field for the LLM to use (relative to now)\n",
                "    simple_file[\"created_datetime_relative\"] = naturaltime(file.created_datetime)\n",
                "    simple_file[\"created_datetime\"] = file.created_datetime.isoformat()\n",
                "\n",
                "    simple_file[\"uuid\"] = str(file.uuid)\n",
                "    return simple_file\n",
                "\n",
                "# Pydantic model to parse the LLM response of selected files\n",
                "\n",
                "class FileSelectionResponse(BaseModel):\n",
                "    reasoning: str = \"\"\n",
                "    selected_files: list[UUID] = []\n",
                "    \n",
                "\n",
                "# Get the list of all the files available\n",
                "\n",
                "files = storage_handler.read_all_items(model_type=\"File\", user_uuid=USER_UUID)\n",
                "\n",
                "# filter where ingest_status == \"complete\"\n",
                "\n",
                "files = [f for f in files if f.ingest_status == \"complete\"]\n",
                "\n",
                "# Simplify the files\n",
                "\n",
                "simplified_files = [simplify_file(f) for f in files]\n",
                "\n",
                "# Create a prompt for the LLM\n",
                "\n",
                "\n",
                "\n",
                "prompt = \"File Selection Instructions: \\n\\n\"\n",
                "\n",
                "prompt += f\"User Query: '{user_query}'\\n\\n\"\n",
                "\n",
                "prompt += \"The following files are available for selection:\\n\\n\"\n",
                "\n",
                "for i, file in enumerate(simplified_files):\n",
                "    prompt += f\"{file[\"uuid\"]} - {json.dumps(file)}\\n\\n\"\n",
                "\n",
                "prompt += \"Please select only relevant file UUIDs to be used in answering this question. \\\n",
                "    If they reference time of a file, use the datetime to infer which file they might mean. \\\n",
                "    Only respond using this following this JSON response schema. Do not repeat the schema, but respond with a JSON response that conforms\\n\\n\"\n",
                "\n",
                "prompt += json.dumps(FileSelectionResponse().model_json_schema(), indent=2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "messages = [\n",
                "    SystemMessage(content=prompt)\n",
                "]\n",
                "\n",
                "resp = llm(messages)\n",
                "\n",
                "print(resp.content)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert the above into pure langchain code\n",
                "\n",
                "\n",
                "\n",
                "from langchain_core.runnables import Runnable, RunnableLambda, RunnablePassthrough, chain\n",
                "from langchain_core.runnables.config import RunnableConfig\n",
                "from langchain_core.prompts import PromptTemplate\n",
                "\n",
                "\n",
                "FILE_SELECTION_PROMPT = \"\"\"\n",
                "File Selection Instructions:\n",
                "\n",
                "Chat History: '{user_query}'\n",
                "\n",
                "The following files are available for selection:\n",
                "\n",
                "{files}\n",
                "\n",
                "Please select only relevant file UUIDs to be used in answering the latest question from the user. \\\n",
                "If they reference time of a file, use the datetime to infer which file they might mean. \\\n",
                "Only respond using this following this JSON response schema. Do not repeat the schema, but respond with a JSON response that conforms\n",
                "\n",
                "{schema}\"\"\"\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "def build_file_selection_chain(\n",
                "        llm: ChatLiteLLM,\n",
                "        storage_handler: ElasticsearchStorageHandler,\n",
                "        user_uuid: str,\n",
                "        env: Settings\n",
                ") -> Runnable:\n",
                "    pass\n",
                "    \n",
                "    \n",
                "    \n",
                "    \n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "redbox-Vh_-Fb0j-py3.11",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
